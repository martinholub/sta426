---
title: "DropSeq Analysis"
author: "Martin Holub"
date: "January 1, 2017"
output: 
  html_document:
    highlight: pygments
---
```{r, eval = FALSE}
------------------------
# Notes

## Useful snippets
  
# module load java
# module load new gcc/4.8.2 r/3.4.0
# module load gcc/4.8.2 star/2.4.2a
# module load samtools

# bsub <script_file> -W 120 -n 8 -R "rusage[mem=4096]"
# bsub -J <"jobname"> -w "done(<other_job_name_or_id>)" -W 640 -n 8 -R "rusage[mem=8192]" < dropseqpipe.sh
# bsub W 120 -n 8 -R "rusage[mem=4096]" "R --vanilla --slave < RNA_seq.R > result.out"
# bmod -W 24:00 JOBID
# bqueues
# bswitch QUEUENAME JOBID
#
# wget "ftp://ftp.ensembl.org/pub/release-90/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.alt.fa.gz"
# wget "ftp://ftp.ensembl.org/pub/release-90/gtf/mus_musculus/Mus_musculus.GRCm38.90.gtf.gz"
# wget "ftp://ftp.ensembl.org/pub/release-90/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz"
# wget "ftp://ftp.ensembl.org/pub/release-90/fasta/mus_musculus/ncrna/Mus_musculus.GRCm38.ncrna.fa.gz"
# wget "ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE63nnn/GSE63472/suppl/GSE63472_mm10_reference_metadata.tar.gz"
# curl "https://portals.broadinstitute.org/single_cell/bulk_data/dronc-seq-single-nucleus-rna-seq-on-mouse-archived-brain/all/594941" -o cfg.txt
# curl -K cfg.txt

# mv $HOME/file.ext $SCRATCH/file.ext
------------------------
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

# Introduction

* Describe the method
* Motivate its development
* Motivate choice
* Describe scope
  - Go from raw fastq data to DGE
  - Do additional filtering on DGE
  - Find PCs by eigenvalue gap method
  - Plot PCA dim reduction and find clusters
* Describe the data
  - Nextera adapter
  - Paired reads UMI+BarCode / sequence
  - Read length

# Overview

In the filtered DGE matrix provided by the authors, there are 13313 cells(nuclei) and 17308 genes. We expect to arrive to similar numbers

The data has been:
**All these using DropSeq Tools**

* Alligned 
  + to mm10 mouse UCSC reference genome using STAR v2.4.0a, recording exonic regions
  
* Preprocessed
  + barcodes filtered for minimum number of transcripts and aggregated (1 edit tolerance)
  + gene counts aggregated on unique UMIs (1 substitution tolerance)
  + reads with low Phred filtered out, trimmed from nucleus barcode (12 bases) and unique molecular index (8 bases)
    
* Exported
  + transcript counts were assembled into digital gene expression (DGE) matrix

**All these using in R**
* the DGE was scaled by total number of UMI counts, multiplied per by mean number of transcripts and log transformed
* effects of number of transcripts and genes detected per nucleus were [regressed out](http://satijalab.org/seurat/cell_cycle_vignette.html)
* Nuclei with less than 200 genes and less than 10000 suable reads were filtered out
* Genes detected in les then 10 nuclei were removed
* Highly variable genes vere selected by fitting gamma-distribution relationship between mean counts and coefficent of variation. Genes with difference in coeficient of variation between empirical and expected lower than 0.2 were dropped. Also genes with mean transcript count lower than 0.005 were dropped.

# Working on Cluster

In this project, we are working with rather large amounts of data. Morover some of the processing we do requires additional large storage and/or plenty of RAM. This is not feasible on most of personal computers and we therefore run most of the code on cluster. For that we use [Euler](https://scicomp.ethz.ch/wiki/Euler), cluster at [ETH](https://www.ethz.ch/de.html). 

### Connecting
We log to Euler by:
``` {bash, eval = FALSE}
ssh USERNAME@euler.ethz.ch
```

### Storage
Euler offers its standard users (e.g. students) two places to deposit their data `home` and `scratch`, briefly, `home` will hold small data indefinitelly while `scratch` holds big data intermittently. (see also [comparsion](https://scicomp.ethz.ch/wiki/Getting_started_with_clusters#Comparison)).

The paths are as follows:

```{bash, eval = FALSE}
home="/cluster/home/USERNAME"
scratch="/cluster/scratch/USERNAME"
```
and we can access them also with `$HOME` and `$SCRATCH`, which is the preferred option.

### Modules
Next we need to load modules required for our analysis and we like to do this anytime we log to the cluster. We can do this by adding corresponding lines to `.bashrc` file in the `home` directory:

``` {bash, eval = FALSE}
mkdir -p $HOME/python/lib64/python3.6/site-packages
export PYTHONPATH=$HOME/python/lib64/python3.6/site-packages:$PYTHONPATH
export PATH=$PYTHONPATH:$PATH
module load new gcc/4.8.2 python/3.6.0
python -m pip install --install-option="--prefix=$HOME/python" rpy2 multiqc

module load java
module load new gcc/4.8.2 r/3.4.0
module load gcc/4.8.2 star/2.4.2a
module load samtools
```

### Software

Couple of additional programs are needed for the analysis, specificaly:

* [Drop-seq_tools-1.12](http://mccarrolllab.com/dropseq/)
  + a suite of java scripts for preprocessing of Drop-seq data
  + it comes already with [Picard](http://broadinstitute.github.io/picard/) which we will heavily rely on for utility functions
  + there is a [cookbook](http://mccarrolllab.com/wp-content/uploads/2016/03/Drop-seqAlignmentCookbookv1.2Jan2016.pdf) available that we will follow closely, [Drop-seq Tutorial & Troubleshooting](http://mccarrolllab.com/drop-seq-troubleshootingtutorial-with-pictures-videos/) will help us as well.
* [dropSeqPipe](https://github.com/Hoohm/dropSeqPipe)
  + a wrapper for the Drop-seq_tools that provides `yaml` and `python` interface and makes uses of [Snakemake](http://snakemake.readthedocs.io/en/stable/) to build reproducible workflow
* [FASTQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
  + a quality control tool for high throughput sequence data
  
All these (and potentional dependencies) are installed following the instructions at respective websites. We place them in `$HOME/Software`. 
  
### Job submission

When submitting jobs to cluster nodes, we use varations of the following example commands:

``` {bash, eval = FALSE}
# for shell scripts
bsub -J "<jobname>" -w "done(other_job_id)" -W 640 -n 8 -R "rusage[mem=8192]" < scriptname.sh
# for R scripts
bsub -W 120 -n 8 -R "rusage[mem=4096]" "R --vanilla --slave < RNA_seq.R > result.out"
```
where (all optional)
- `-W` is the time limit in minutes,
- `-n` is the number of threads
- `-J` is the job name
- `-R` is the memory requirement per thread

# Reference Index

To quantify transcript abundances we need a reference genome to aling our reads to. For our purposes, we use files prepared by the authors of the DropSeq method ([Macosko et al., 2015](http://linkinghub.elsevier.com/retrieve/pii/S0092-8674(15)00549-8)), that can be dowloaded [here](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE63nnn/GSE63472/suppl/).

In later steps we will be using STAR for alignement. To this end, we must first generate STAR index. This is done with:

```{bash, eval = FALSE}
genome_dir="/cluster/scratch/mholub/reference/mm10_index"
genome_fasta="/cluster/scratch/mholub/reference/mm10/mm10.fasta"
genome_gtf="/cluster/scratch/mholub/reference/mm10/mm10.gtf"
read_length=50

echo "'STAR version:\n'"
STAR --version
STAR --runMode genomeGenerate --runThreadN 8 --genomeDir $genome_dir \
--genomeFastaFiles $genome_fasta --sjdbGTFfile $genome_gtf --sjdbOverhang $read_length - 1
```

Where the `read_length` is determined from the fastq files with:

```{python, eval = FALSE}
def get_mean_read_length(wildcards):
	total_length = 0
	n = 1000000
	with gzip.open(wildcards + '_tagged_unmapped.fastq.gz' , 'r') as reads:
		fourthlines = islice(reads, 1, n, 4)
		for line in fourthlines:
			read = line.strip()
			total_length += len(read)
	return(int(total_length/(n/4)))
```

Note thar read lenght must be identical for construction of genome index and alignement.

# Download

The authors have deposited the data at the [SingleCell Portal](https://portals.broadinstitute.org/single_cell/study/dronc-seq-single-nucleus-rna-seq-on-mouse-archived-brain). These can be downloaded in bulk using:

```{bash, eval = FALSE}
curl "https://portals.broadinstitute.org/single_cell/bulk_data/dronc-seq-single-nucleus-rna-seq-on-mouse-archived-brain/all/000000" -o cfg.txt
curl -K cfg.txt -o $SCRATCH
```

Note that the zeros at the end of the link indicate unique key for download that you will have to generate and that this is valid only for 30 minutes.

### Rename

For convenience we remove redundant suffix in the filename such that `file_001.fastq.gz` becomes `file.fastq.gz` with:

```{bash, eval = FALSE}
for fname in *.fastq.gz
do
  mv "$fname" "$(echo "$fname" | sed -r 's/_001//')"
done
```

# Preprocessing

In the following I walk you through the steps required for reproducing the analysis presented in the paper [Massively parallel single-nucleus RNA-seq with DroNc-seq](http://dx.doi.org/10.1038/nmeth.4407) (Habib et al., 2017). The preprocessing is illustrated in the following figure:

![Preprocesing overview](C:\Users\marti\git\sta426\project\pics\dropseq_flowchart.png)


## dropSeqPipe

We first setup the pipeline for preprocessing. We do that following the [Wiki](https://github.com/Hoohm/dropSeqPipe/wiki). The pipeline is implemented using [Snakemake](http://snakemake.readthedocs.io/en/stable/) and we control it's behavior by two configuration files, `config.yaml` and `local.yaml`. The content of these files will be more closely described when we go over individual steps of the pipeline. Remember that the pipeline is just a convenience wrapper for underlying DropSeq Tools scripts develeped at [McCarroll Lab](http://mccarrolllab.com/dropseq/) and running in Java.

The `config.yaml` file is:

```{yaml, eval = FALSE}
Samples:
    0_2a_S1:
        fraction: 0.07
        expected_cells: 1400
    0_1b_S2:
        fraction: 0.07
        expected_cells: 1400
    0_2b_S2:
        fraction: 0.07
        expected_cells: 1400
    0_1a_S1:
        fraction: 0.07
        expected_cells: 1400
    mPFC2_S2:
        fraction: 0.07
        expected_cells: 1400
    PFC1_S2:
        fraction: 0.07
        expected_cells: 1400
    PFC3_S2:
        fraction: 0.07
        expected_cells: 1400
    HP3_S1:
        fraction: 0.07
        expected_cells: 1400
    HP1_S1:
        fraction: 0.07
        expected_cells: 1400
    mHP2_S1:
        fraction: 0.07
        expected_cells: 1400
GENOMEREF: $SCRATCH/reference/mm10/mm10.fasta
REFFLAT: $SCRATCH/reference/mm10/mm10.refFlat
METAREF: $SCRATCH/reference/mm10/STAR_index
RRNAINTERVALS: $SCRATCH/reference/mm10/mm10.rRNA.intervals
GTF: $SCRATCH/reference/mm10/mm10.gtf
CORES: 12
SPECIES:
    - MOUSE
GLOBAL:
    5PrimeSmartAdapter: AAGCAGTGGTATCAACGCAGAGT
    data_type: singleCell
    allowed_aligner_mismatch: 10
    min_count_per_umi: 1
    min_genes_per_cell: 200
    min_umis_per_cell: 10000
    Cell_barcode:
        start: 1
        end: 12
        min_quality: 10
        num_below_quality: 1
    UMI:
        start: 13
        end: 20
        min_quality: 10
        num_below_quality: 1
```

Where paramteres in `Samples` section were selected based on experimental parameters. Specifically the `fraction` is given by the number of nuclei per ml (300'000) and number of droplets per ml (4'500'000) giving Poisson loading parameter of 0.07. 

The Expected number of cells can be determined from `knee plot`, example of which is given under (note that this one becomes available only atfert allignement and initial preprocessing steps). Ideally, we see a clear infelction point. It makes little sense to include more cells then the location of inflection point as these mostly  represent empty droplets. 


![Preprocesing overview](C:\Users\marti\git\sta426\project\plots\0_2a_S1_knee_plot.pdf)

Ideally, we see a clear infelction point. It makes little sense to include more cells then the location of inflection point as these represent empty droplets.

Parameters in `GLOBAL` are then selected based on information from DropSeq cookbook and the Methods section of the reproduced paper. 

1) It would have been advantageous to do some filtering steps at this point as the resulting DGEs would be easier to work with, but from the paper it seems that authors did this filtering only in R and we stick to this approach.

2) In contrast to the procedure described in paper, we decide to filter on minimal number of genes and transcripts per cell already in this step. Authors reported doing so only in R, but the approaches should be equivalent. Morover, the approach we adopt prevents dragging lot of unused data along. 

The `local.yaml` file is:

``` {yaml, eval = FALSE}
TMPDIR: $SCRATCH/temp
PICARD: $HOME/Software/Drop-seq_tools-1.12/3rdParty/picard/picard.jar
DROPSEQ: $HOME/Software/Drop-seq_tools-1.12
STAREXEC: /cluster/apps/star/2.4.2a/x86_64/STAR
FASTQCEXEC: $HOME/Software/FastQC/fastqc
CORES: 8
READLENGTH: 50
TRIMEXEC: $HOME/Software/Trimmomatic-0.36/trimmomatic-0.36.jar
```

Where read length must be consistent across all samples and index.

In the following, we go over individual piepline steps. The sections are named by the corresponding DropSeq Tool or the name of the Picard's function. Parameters in curly braces or those given in capitals correspond to definitions in the two `.yaml` configuration files. 

### 0. BCL to FASTQ

In this step, `BCL` files were converted to `FASTQ` files. This can be done for example with [bcl2fastq](https://support.illumina.com/downloads/bcl2fastq_conversion_software_184.html). As we have directly downloaded the `.fastq` files, we may omit it.

### 1. FastqToSam

Converts raw input files from `{sample}.fastq.gz` to `{sample}_unaligned.bam` using Picard's `FastqToSam`.

``` {python, eval = FALSE}
rule fastq_to_sam:
	"""Create an empty bam file linking cell/UMI barcodes to reads"""
	input:
		r1='{sample}_R1.fastq.gz',
		r2='{sample}_R2.fastq.gz'
	output:
		temp('{sample}_unaligned.bam')
	threads: CORES
	shell:
		"""java -Djava.io.tmpdir={TMPDIR} -Xmx8g -Xms4096m -XX:ParallelGCThreads={CORES} -jar {PICARD} FastqToSam\
		F1={input.r1}\
		F2={input.r2}\
		SM=DS O={output}"""
```

### 2. TagBamWithReadSequenceExtended

As described the DroNc-seq protocol sequences paired-end reads of different length. As a result we end up with 2 files for each sample, one holds barcodes, the other the actual RNA sequence. In our, case the shorter 20-bp sequence corresponds to the cell barcode (base 1-12) and molecular barcode (base 13-20) whereas the longer 50-bp sequence is the actual RNA.

In the previous step these two files were aggregated into one. In this step, we then tag the molecular and cell barcodes producing `{sample}_tagged_unmapped.bam` file. If more than `num_below_quality` bases have Phred score bellow `min_quality`, the read pair is flagged.

````{python, eval = FALSE}
rule stage1:
	input: '{sample}_unaligned.bam'
	output: '{sample}_tagged_unmapped.bam'
	params:
		BC_summary = 'logs/{sample}_CELL_barcode.txt',
		UMI_summary = 'logs/{sample}_UMI_barcode.txt',
		start_trim = 'logs/{sample}_start_trim.txt',
		polyA_trim = 'logs/{sample}_polyA_trim.txt',
		BC_start = config['GLOBAL']['Cell_barcode']['start'],
		BC_end = config['GLOBAL']['Cell_barcode']['end'],
		BC_min_quality = config['GLOBAL']['Cell_barcode']['min_quality'],
		BC_min_quality_num = config['GLOBAL']['Cell_barcode']['num_below_quality'],
		UMI_start = config['GLOBAL']['UMI']['start'],
		UMI_end = config['GLOBAL']['UMI']['end'],
		UMI_min_quality = config['GLOBAL']['UMI']['min_quality'],
		UMI_min_quality_num = config['GLOBAL']['Cell_barcode']['num_below_quality'],
		SmartAdapter = config['GLOBAL']['5PrimeSmartAdapter']
	threads: CORES
	shell:
		"""{DROPSEQ}/TagBamWithReadSequenceExtended\
		SUMMARY={params.BC_summary}\
		BASE_RANGE={params.BC_start}-{params.BC_end}\
		BASE_QUALITY={params.BC_min_quality}\
		BARCODED_READ=1\
		DISCARD_READ=false\
		TAG_NAME=XC\
		NUM_BASES_BELOW_QUALITY={params.BC_min_quality_num}\
		INPUT={input}\
		OUTPUT=/dev/stdout COMPRESSION_LEVEL=0 |\
		\
		{DROPSEQ}/TagBamWithReadSequenceExtended\
		SUMMARY={params.UMI_summary}\
		BASE_RANGE={params.UMI_start}-{params.UMI_end}\
		BASE_QUALITY={params.UMI_min_quality}\
		BARCODED_READ=1\
		DISCARD_READ=true\
		TAG_NAME=XM\
		NUM_BASES_BELOW_QUALITY={params.UMI_min_quality_num}\
		INPUT=/dev/stdin\
		OUTPUT=/dev/stdout COMPRESSION_LEVEL=0 |\
		\
		...
		\
		...
		\
		....
		"""
```

Where `...` indicate calls explained in the three sections bellow.

### 3. FilterBAM

In this step, the read pairs that were flagged as low quality are discared to produce `{sample}_tagged_unmapped_filtered.bam` file.

...

```{shell, eval = FALSE}
{DROPSEQ}/FilterBAM TAG_REJECT=XQ\
		INPUT=/dev/stdin\
		OUTPUT=/dev/stdout COMPRESSION_LEVEL=0 |\
```

...

### 4. TrimStartingSequence

Here we trim away possible traces of Ilumina Nextera SMART adapter from the 5'end. If there is at least 5 contiguous bases with no mismatch to the indicated `5PrimeSmartAdapter` sequence, we clip them off the read. We apply the konwledge that the barcode has 20 bp where the first 12 bp correspond to cell barcode and the remaining 8 bp to molecular barcode (UMI).

...
```{shell, eval = FALSE}
{DROPSEQ}/TrimStartingSequence\
		OUTPUT_SUMMARY={params.start_trim}\
		SEQUENCE={params.SmartAdapter}\
		MISMATCHES=0\
		NUM_BASES={starttrim_length}\
		INPUT=/dev/stdin\
		OUTPUT=/dev/stdout COMPRESSION_LEVEL=0 |\
```
...

### 5. PolyATrimmer

Now we trim away sequences from the 3'end that consist of at least 6 contiguous A's with no mismatches.
...

```{shell, eval = FALSE}
{DROPSEQ}/PolyATrimmer\
		OUTPUT_SUMMARY={params.polyA_trim}\
		MISMATCHES=0\
		NUM_BASES=6\
		OUTPUT={output}\
		INPUT=/dev/stdin
		
```

### 6. SamToFastq

We then run Picard's function `SamToFastq` to convert the filtered files back to `fastq.gz` file format.

``` {python,  eval = FALSE}

rule sam_to_fastq:
	input: '{sample}_tagged_unmapped.bam'
	output: '{sample}_tagged_unmapped.fastq.gz'
	threads: CORES
	shell:
		"""java -Xmx8g -Xms4096m -XX:ParallelGCThreads={CORES} -jar -Djava.io.tmpdir={TMPDIR}	{PICARD} SamToFastq\
		INPUT={input}\
		FASTQ=/dev/stdout COMPRESSION_LEVEL=0|\
		gzip > {output}"""

```

# Alignement

At this point we are ready to aling our reads to reference. There are multiple ways how to do it, but we stick to the approach used by the authors and use STAR. 

The `snake` / `bash` code that does this is shown bellow:

``` {python, eval = FALSE}
# Configfile
configfile: 'config.yaml'

STAREXEC = config['STAREXEC']
METAREF = config['METAREF']
CORES = config['CORES']
GTF = config['GTF']
MISMATCH = config['GLOBAL']['allowed_aligner_mismatch']
READLENGTH = config['READ_LENGTH']

rule STAR_align:
	input:  '{sample}_tagged_unmapped.fastq.gz'
	output: sam = 'logs/{sample}.Aligned.out.sam'
	params:
		prefix = '{sample}.',
		mismatch = MISMATCH,
		mean_read_length = READLENGTH
	threads: CORES
	shell:"""{STAREXEC}\
			--genomeDir {METAREF}\
			--sjdbGTFfile {GTF}\
			--readFilesCommand zcat\
			--runThreadN {CORES}\
			--readFilesIn {input}\
			--outFileNamePrefix logs/{params.prefix}\
			--sjdbOverhang {params.mean_read_length}\
			--twopassMode Basic\
			--outFilterScoreMinOverLread 0.3\
			--outFilterMatchNminOverLread 0\
			--outFilterMismatchNoverLmax 0.3"""
```

We then continue in the preprocessing pipeline.

### 7. SortSam

A Picard tool to sort the `{sample}.Aligned.out.sam` file in queryname order and convert to binary format.

``` {python, eval = FALSE}
rule sort:
	input:
		samples = '{sample}.Aligned.sam'
	output: temp('{sample}_Aligned_sorted.sam')
	threads: CORES
	shell:
		"""java	-Djava.io.tmpdir={TMPDIR} -Dsamjdk.buffer_size=131072 -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 \
		-XX:ParallelGCThreads={CORES} -Xmx8g -Xms4096m -jar {PICARD} SortSam\
		INPUT={input}\
		OUTPUT={output}\
		SORT_ORDER=queryname\
		TMP_DIR={TMPDIR}"""

```

### 8. MergeBamAlignment

A Picard tool to merge the sorted STAR alignement with unaligned BAM file that has been previously tagged with molecular and cell barcodes. This recovers the read identity. Only primary alignements are considered.

``` {python, eval = FALSE}
rule stage3:
	input:	unmapped = '{sample}_tagged_unmapped.bam',
			mapped = '{sample}_Aligned_sorted.sam'
	output: temp('{sample}_gene_exon_tagged.bam')
	threads: CORES
	shell:
		"""java -Djava.io.tmpdir={TMPDIR} -Xmx8g -Xms4096m -XX:ParallelGCThreads={CORES} -jar {PICARD} MergeBamAlignment\
		REFERENCE_SEQUENCE={GENOMEREF}\
		UNMAPPED_BAM={input.unmapped}\
		ALIGNED_BAM={input.mapped}\
		INCLUDE_SECONDARY_ALIGNMENTS=false\
		PAIRED_RUN=false\
		OUTPUT=/dev/stdout COMPRESSION_LEVEL=0|\

    ...
  
		"""
```

### 9. TagReadWithGeneExon

...

This script tags the reads with "GE" BAM tag if they are known to overlap an exon region of a gene as defined in annotation file (`mm10.gtf` or `mm10.refFlat`).

``` {shell, eval = FALSE}
  export _JAVA_OPTIONS="-XX:ParallelGCThreads=8"
	{DROPSEQ}/TagReadWithGeneExon\
	OUTPUT={output}\
	INPUT=/dev/stdin\
	ANNOTATIONS_FILE={REFFLAT}\
	TAG=GE\
	CREATE_INDEX=true
```


### 10. DetectBarcodeSynthesisErrors

The program checks the identifier sequence (cell + molecular barcode). Specifically, it looks at the distribution skew in UMIs (molecular barcode) associated with given cell barcode that could have been caused by incomplete synthesis of the cell barcode (e.g. length of only 11 bases), that would in turn result in high percentage of T at the last UMI position (see figure).

![Cell and Molecular Barcode Illustration](C:\Users\marti\git\sta426\project\pics\SAoHTGaTD\detect_barcode_errors.png)

Also possible matches with any of the PCR primers are checked and if found, the barcodes and corresponding reads are dropped.

```{python, eval = FALSE}
rule bead_errors_metrics:
	input: '{sample}_gene_exon_tagged.bam'
	output: '{sample}_final.bam'
	threads: CORES
	params:
		out_stats = 'logs/{sample}_synthesis_stats.txt',
		summary = 'logs/{sample}_synthesis_stats_summary.txt',
		barcodes = lambda wildcards: config['Samples'][wildcards.sample]['expected_cells'] * 2,
		cells = lambda wildcards: config['Samples'][wildcards.sample]['expected_cells'],
		metrics = 'logs/{sample}_rna_metrics.txt'
	shell:
		"""{DROPSEQ}/DetectBeadSynthesisErrors\
		INPUT={input}\
		OUTPUT={output}\
		OUTPUT_STATS={params.out_stats}\
		SUMMARY={params.summary}\
		NUM_BARCODES={params.barcodes}\
		PRIMER_SEQUENCE=AAGCAGTGGTATCAACGCAGAGTAC;
		{DROPSEQ}/SingleCellRnaSeqMetricsCollector\
		INPUT={input}\
		OUTPUT={params.metrics}\
		ANNOTATIONS_FILE={REFFLAT}\
		NUM_CORE_BARCODES={params.cells}\
		RIBOSOMAL_INTERVALS={RRNAINTERVALS}
		"""

```

**At this point the raw data preprocessing and alignement is finished.** The reads have been transformed from paired-end to single-end with corresponding cell and molecular barcodes. They have been also filtered and some barcodes were error-corrected.


# Extraction of Digital Gene Expression matrix

Now we are ready to extract DGE matrix from the data. This is done by invking `DigitalExpression` function. To eliminate multi-mapping reads, we require the quality to be at least 10. We collapse UMI barcodes within a Hamming distance of 1 to account for possible errors during amplification.

Further we may impose minimal thresholds on number of genes per cell and number of reads per UMI  to remove empty droplets and effectively reduce the size of resulting matrix.

The corresponding `snake` snippet looks like this:
``` {python, eval = FALSE}
"""Extract expression fof single species."""

configfile: 'config.yaml'
DROPSEQ = config['DROPSEQ']

rule all:
	input: 
		expand('summary/{sample}_expression_matrix.txt', sample=config['Samples']),
		expand('logs/{sample}_umi_per_gene.tsv', sample=config['Samples']),
		
rule extract_expression:
	input: '{sample}_final.bam'
	output: 'summary/{sample}_expression_matrix.txt.gz'
	params:
		sample = '{sample}',
		cells = lambda wildcards: config['Samples'][wildcards.sample]['expected_cells'],
		count_per_umi = config['GLOBAL']['min_count_per_umi'],
		genes_per_cell = config['GLOBAL']['min_genes_per_cell'],
		umis_per_cell = config['GLOBAL']['min_umis_per_cell']
	shell:
		"""{DROPSEQ}/DigitalExpression\
		I={input}\
		O={output}\
		SUMMARY=summary/{params.sample}_dge.summary.txt \
		CELL_BC_FILE=summary/{params.sample}_barcodes.csv \
		MIN_BC_READ_THRESHOLD={params.count_per_umi} \
		MIN_NUM_GENES_PER_CELL={params.genes_per_cell} \
		MIN_NUM_TRANSCRIPTS_PER_CELL={params.umis_per_cell} \
		NUM_CORE_BARCODES={params.cells}"""
```

We also extract other useful data:

```{python, eval = FALSE}
rule extract_umi_per_gene:
	input: '{sample}_final.bam'
	output: 'logs/{sample}_umi_per_gene.tsv'
	params:
		sample = '{sample}'
	shell:
		"""{DROPSEQ}/GatherMolecularBarcodeDistributionByGene\
		I={input}\
		O={output}\
		CELL_BC_FILE=summary/{params.sample}_barcodes.csv"""
```

Resulting is a DGE matrix that we will work with further.

# SEURAT

**For following processing we can already move from cluster to local machine.**

Seurat is a R package for DropSeq data analysis. We will use it to further process data and to plot some figures.

Setup basepath:

``` {r basepath,  eval = FALSE}
setwd("../git/sta426/project")
basepath <- getwd()
```

Get paths to files and sample names:

``` {r filePaths}
mouse.datafiles <- list.files(path = file.path(basepath, 'summary/27122017_filter'), pattern = "*_expression_matrix.txt$", full.names = TRUE)
mouse.annofiles <- list.files(path = file.path(basepath, 'summary/27122017_filter'), pattern = "*_dge.summary.txt$", full.names = TRUE)
mouse.umigenefiles <- list.files(path = file.path(basepath, 'logs'), pattern = "*_umi_per_gene.tsv$", full.names = TRUE)
mouse <- list()
mouse$samples <- gsub("(.*)_S[12]_dge.summary.txt$", "\\1", basename(mouse.annofiles))
```

## Load, Scale and Normalize & Remove genes with low expression

*Construct Seurat Object while retaining sample identity.* 

To quote the paper:
"The DGE matrix was scaled by total UMI counts, multiplied by the mean number of transcripts (calculated for each data set separately), and the values were log transformed".

From observation and from available documentation, we infer that the scaling by total number of UMI counts happens implicitly and we thus need to specify only the latter two steps/.

Additionally we filter out genes with low and too rare expression.

To quote the paper: 
" A gene is considered detected in a cell if it has at least two unique UMIs (transcripts) associated with it. For each analysis, genes were removed that 
were detected in less than 10 nuclei. "

``` {r constructSeurat}
for (i in 1:length(mouse$samples)){
  
  counts <- read.table(mouse.datafiles[i] , sep = "\t", header = TRUE)
  anno <- read.table(mouse.annofiles[i] , sep = "\t", header = TRUE)
  umi_per_gene <-  read.table(mouse.umigenefiles[i] , sep = "\t", header = TRUE)
  
  num_obs <- aggregate(Num_Obs ~ Gene, data = umi_per_gene, sum)
  num_umis <- aggregate(Molecular_Barcode ~ Gene, data = umi_per_gene, length)
  num_cells <- aggregate(Cell.Barcode ~ Gene, data = umi_per_gene, length)
  idx <- setNames((num_umis$Molecular_Barcode > 2) & (num_cells$Cell.Barcode > 10), levels(num_cells$Gene))
  num_umis <- num_umis$Molecular_Barcode[idx]
  idx_match <- idx[match(counts[ , 1], names(idx))]
  
  #move cell barcodes and gene names out of matrix
  # Make sure unqiue barcodes across samples by prepending sample name
  rownames(counts) <- counts[ , 1]
  counts <- counts[, -1]
  colnames(counts) <- paste0(mouse$samples[i], ".", colnames(counts))
  counts <- counts[idx_match, ]
  
  rownames(anno) <- paste0(mouse$samples[i], ".", anno[ , 1])
  anno <- anno[ ,-1]
  anno <- anno[match(colnames(counts), rownames(anno)), ]
  
  mean_count <- mean(anno$NUM_TRANSCRIPTS, na.rm = TRUE)
  
  sparse_counts <- Matrix::Matrix(data.matrix(counts), sparse = TRUE)
  
  
  if (i ==1) {
    
    sObj <- Seurat::CreateSeuratObject(raw.data = sparse_counts, project = 'DroNcSeq', names.delim = ".",
                                       normalization.method = NULL)
    sObj <- Seurat::AddMetaData(sObj, anno, colnames(anno))
    sObj@meta.data$orig.ident <- factor(mouse$samples[i])
    sObj <- Seurat::NormalizeData(object = sObj, normalization.method = "LogNormalize", scale.factor = mean_count)
    
  } else {
    sObj_temp <- Seurat::CreateSeuratObject(raw.data = sparse_counts, project = 'DroNcSeq', names.delim = ".",
                                            normalization.method = NULL)
    sObj <- Seurat::AddMetaData(sObj, anno, colnames(anno))
    sObj_temp@meta.data$orig.ident <- factor(mouse$samples[i])
    sObj_temp <- Seurat::NormalizeData(object = sObj_temp, normalization.method = "LogNormalize", scale.factor = mean_count)
    
    sObj <- Seurat::MergeSeurat(sObj, sObj_temp)
    remove(sObj_temp)
  }
}
mouse$sObj <- sObj
remove(sObj)
```

Do quick sanity check.

```{r check_sample}
idxer <- sample(nrow(mouse$sObj@data), 7)
mouse$sObj@scale.data[idxer, 1:7]
mouse$sObj@data[idxer, 1:7]
mouse$sObj@raw.data[idxer, 1:7]
```


``` {r violinPlot}
Seurat::VlnPlot(object = mouse$sObj, features.plot = c("nGene", "nUMI"), nCol = 2, group.by = "orig.ident", y.log = TRUE)
```

``` {r GenePlot}
par(mfrow = c(1, 2))
#Seurat::GenePlot(object = mouse$sObj, gene1 = "nUMI", gene2 = "percent.aa")
Seurat::GenePlot(object = mouse$sObj, gene1 = "nUMI", gene2 = "nGene")
```


## Regress out

To quote authors of the paper: "To reduce the effects of library quality and complexity on cluster identity, a linear model was used to regress out effects of the number of transcripts and genes detected per nucleus (using the 'RegressOut' function in the Seurat software package)."

To quote Seurat: "the `RegressOut` function has been deprecated, and replaced with the vars.to.regress argument in `ScaleData`."

In brevity, we atempt to "regress-out" uninteresant sources of variation , including technical noise, batch effects, cell-cycle stage. This is done by learning a linear model to predict gene expression based on user-defined variables. 

```{r regress_out}
mouse$sObj <- Seurat::ScaleData(mouse$sObj, vars.to.regress = c('nGene', 'nUMI'), do.scale = FALSE, do.center = FALSE)
```


## Find variable genes

"To select highly variable genes, we fit a relationship between mean counts and coefficient of variation using a gamma distribution on the data from all of the genes and ranked genes by the extent of excess variation as a function of their mean expression (using a threshold of at least 0.2  difference in the coefficient of variation between the empirical and the expected and a minimal mean transcript count of 0.005)."

* note that LogVMR is logarithm of variance to mean ratio, which is the logarith of coeficient of varation and note also that we define the threshold in non-log space, thus consistently with paper. (see Seurat::ExpMean)

```{r variable_genes}
mouse$sObj <- Seurat::FindVariableGenes(mouse$sObj, mean.function = Seurat::ExpMean, dispersion.function = Seurat::LogVMR, 
                   x.low.cutoff = 0.005,  y.cutoff = 0.2, num.bin = 50)
```
## Dimensionality reduction - PCA

Do dimensionality reduction. Extract more PCs than you are likely to need. 

Quoting the paper:
"We used a DGE matrix consisting only of variable genes as defined above, scaled and log-transformed, and then reduced its dimensions with PCA"

``` {r PCA_extract}
#mouse$sObj@dr$pca <- rsvd::rpca(mouse)
mouse$sObj <- Seurat::RunPCA(mouse$sObj, pc.genes = mouse$sObj@var.genes, pcs.compute = 50, genes.print = 5)
```

Quoting: "We [...] chose the most significant principal components (or PCs) based on the largest eigen value gap ...".

This can be done by looking for PCs that have high enrichement at low p-values (dashed line represents uniform distribution).

```{r pickPCs}
mouse$sObj <- Seurat::JackStraw(mouse$sObj, num.pc = 50, num.replicate = 100, do.print = TRUE)
Seurat::JackStrawPlot(mouse$sObj, PCs = 1: 9, nCol = 3)
```

We can also check the explained variance plot, that can yield simialr insight.

``` {r explainedVariance}
Seurat::PCElbowPlot(mouse$sObj)

```

From the plots, it looks like 4-8 PCs is a reasonable choice. Let's take 4.

## Dimensionality Reduction - tSNE

``` {r RunTSNE}
mouse$sObj <- Seurat::RunTSNE(mouse$sObj, reduction.use = "pca", dims.use = 1:4, dim.embed = 2)
```

```{r plotTSNE}
Seurat::TSNEPlot(mouse$sObj, do.label = TRUE)
```

Seurat::NegBinomRegDETest()
Seurat::AverageExpression()
Seurat::GetClusters ()


# Additional Filtering on DGE

Authors report additional processing on the extacted DGE 

### ALTERNATIVE: Using SALMON for Allignement

```{r, eval = FALSE}
# Prepare paths
data_dir <- "/cluster/scratch/mholub"
output_dir <- "/cluster/home/mholub/output"
reference_dir <- "/cluster/scratch/mholub/reference"
path_to_fastqc <- "/cluster/home/mholub/Software/FastQC/fastqc"
path_to_salmon <- "/cluster/home/mholub/Software/Salmon-0.8.2_linux_x86_64/bin/salmon"
path_to_samtools <- "/cluster/home/mholub/Software/samtools/bin/samtools"


# Find files
fastq.files <- list.files(path = data_dir, pattern = "unmapped.fastq.gz$", full.names = TRUE)
(cmd <- sprintf("head %s", fastq.files[1]))
system(cmd)

## Index reference transcriptome for use with Salmon
reference_path <- paste0(scratch, '/reference/mm10/mm10.fasta')
index_path <- paste0(scratch, '/reference/mm10_index_salmon/mm10.sidx')

(cmd <- sprintf("%s index -t %s -i %s -p 8 -k 23", 
                path_to_salmon,
                reference_path,
                index_path))
system(cmd)

# Quantify transcript abundances
gtf_path <- paste0(reference_dir, "/mm10.gtf")
out_f <- gsub("\\..*", "", gsub("(.*)/([^\\.]+[\\.])","\\2", fastq.files[1]))
salmon_res <- sprintf("%s/%s",output_dir, out_f)
(cmd <- sprintf(
                "%s quant -i %s -l A -r %s -o %s -p 8 -k 23 -g %s --writeMappings %s", # We have single end read as the other end is jut UMI and Barcode
                #"%s quant -i %s -l A -1 %s -2 %s -o %s -p 4 -k 19", #we have paired reads
                path_to_salmon,
                index_path,
                fastq.files,
                salmon_res,
                gtf_path,
                data_dir))
system(cmd)

# Convert sam file to binary bam file and sort alignments
sbam_file <- paste0(data_dir, 'sometext')
(cmd <- sprintf("%s view -b %s | %s sort -T tmp -O bam - > %s",
                path_to_samtools, 
                paste0(sbam_file, ".sam"),
                path_to_samtools, 
                paste0(sbam_file, ".bam")))
system(cmd)

# Index bam file
(cmd <- sprintf("%s index %s/sometext.bam", 
                path_to_samtools, 
                output_dir))
system(cmd)
```
